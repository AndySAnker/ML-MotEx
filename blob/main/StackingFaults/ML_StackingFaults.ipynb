{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-StackingFaults.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOftyeIsM4rxSHRdM/IdG9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndySAnker/ML-MotEx/blob/main/blob/main/StackingFaults/ML_StackingFaults.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to ML-MotEx\n",
        "Github: https://github.com/AndySAnker/ML-MotEx/tree/main/StackingFaults\n",
        "\n",
        "Paper: Characterisation of intergrowth in metal oxide materials using structure-mining: the case of Î³-MnO2\n",
        "\n",
        "Questions: andy@chem.ku.dk & nima@chem.ku.dk\n",
        "\n",
        "Use this script to use ML-MotEx to extract a stacking fault distribution from a dataset. This script does both handle fits made on Pair Distribution Function (PDF) data and Powder X-ray Diffraction (PXRD) data.\n",
        "\n",
        "The script first import packages and defines functions.\n",
        "\n",
        "Section 1: We import the stacking fault sequences and their Rwp values\n",
        "\n",
        "Section 2: We train a gradient boosted decision tree to predict the Rwp value from the stacking fault sequence\n",
        "\n",
        "Section 3: We plot the importance of the layers and why they are important. Do the model like specific layers?\n",
        "\n",
        "Section 4: We extract the information in two lists, good atoms/bad layers. We calculate the stacking fault distribution.\n",
        "\n",
        "# Import modules and set seed parameters"
      ],
      "metadata": {
        "id": "T11_1lqswfhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFc0SvpzwVOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time, random, shap, sklearn\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "random.seed(14)\n",
        "np.random.seed(14)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "1_bEsFKNwph2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_XGBoost_XY(X_train, y_train, learning_rate, max_depth, n_estimators, n_jobs, gamma, min_child_weight, base_score, seed, xgb_model=None):\n",
        "    start_time = time.time()\n",
        "    model = xgb.XGBRegressor(learning_rate = learning_rate, max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_jobs, gamma=gamma, min_child_weight=min_child_weight, base_score=base_score, random_state=seed)\n",
        "    model.fit(X_train, y_train, xgb_model=xgb_model)\n",
        "    print(\"Total execution time: %.3fs\"%(time.time()-start_time))\n",
        "    print (\"Training Succeeded\")\n",
        "    return model\n",
        "    \n",
        "def Validate_XGBoost(model, X_val, y_val):\n",
        "    print (\"Giving an estimate of the accuracy of the model\")\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "    print(\"RMSE: %f\" % (rmse))\n",
        "    return rmse\n",
        "\n",
        "def shap_essential_figure(model, X_train):\n",
        "    plt.clf()\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_train) #to explain every prediction\n",
        "    shap.summary_plot(shap_values, X_train, feature_names=names[2:], max_display=20, show=False) # to plot these explanations\n",
        "    return explainer, shap_values"
      ],
      "metadata": {
        "id": "k-S189yBwptQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Import data ready for machine learning"
      ],
      "metadata": {
        "id": "vEarwab-wudE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName = \"Training_Data/info_Intergrowth_ML_XRD_Diamond2h.txt\"\n",
        "Layers = 250\n",
        "\n",
        "names = ['index', 'Rwp', 'Pb', 'Pr', 'percent_B', 'percent_R']+['Block_sequence'+str(i) for i in range(1, Layers+1)]+['Intergrowth_sequence'+str(i) for i in range(1, Layers+1)]\n",
        "data = pd.read_csv(fileName, names=names, delimiter=\" \") # Read data file\n",
        "data = data.drop(data.columns[-1],axis=1) # Drop the last column with NANS\n",
        "data = data.drop(data.columns[0],axis=1) # Drop index\n",
        "data = data.replace(['P','R'], [0, 1]) # 0 is P, 1 is R\n",
        "\n",
        "y = data['Rwp'] # Rwp is labels\n",
        "X = data.drop(data.columns[0], axis=1) # Drop labels\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) # Split the data into training and validation set"
      ],
      "metadata": {
        "id": "Q_22ahqqwuwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check that data is imported correct\n",
        "Check 1: Is the number of training data 4 times larger than validation data?\n",
        "\n",
        "Check 2: Print y_train and make sure it is the Rwp values of the fits.\n",
        "\n",
        "Check 3: Print X_train and make sure that the first 5 entries are index, Pb, Pr, percent_B, percent_R. The next should be Block_sequence0, Block_sequence1, etc... the last should be Intergrowth_sequence0, Intergrowth_sequence1, etc..."
      ],
      "metadata": {
        "id": "xDYr_9WLw1gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 1\n",
        "print (\"Number of Training Data:\", len(X_train))\n",
        "print (\"Number of Validation Data:\", len(X_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LG4_GhE0w1tx",
        "outputId": "463dd6f3-a8f7-445f-f571-64d7fe02c86e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-27d42bc39f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Training Data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Validation Data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 2\n",
        "print (y_train.head())\n",
        "print ()\n",
        "print (\"Stastical information about the Rwp distribution\")\n",
        "print (y_train.describe())"
      ],
      "metadata": {
        "id": "MmVtY9srw5eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 3\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "-37Fer86w8ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Train the gradient boosted decision tree algorithm\n",
        "The parameters can be changed in order to get the lowest possible root mean squared error (RMSE). Normally only learning rate and max_depth is changed. Max depth can only be changed as integers."
      ],
      "metadata": {
        "id": "_vdZ0b1IxARw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.25 # 0.1 is default - Boosting learning rate\n",
        "max_depth = 3 # 3 is default - Maximum tree depth for base learners.\n",
        "n_estimators = 100 # 100 is default - Number of trees to fit. \n",
        "n_jobs = 1 # 1 is default - Number of parallel threads used to run xgboost.\n",
        "gamma = 0 # 0 is default - Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
        "min_child_weight = 1 # 1 is default - Minimum sum of instance weight(hessian) needed in a child.\n",
        "base_score = 0.5 # 0.5 is default - The initial prediction score of all instances, global bias.\n",
        "random_state = 0 # 0 is default - seeding\n",
        "\n",
        "# Set up a ML algorithm based on the loaded permutations\n",
        "model = Train_XGBoost_XY(X_train, y_train, learning_rate=lr, max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_jobs, gamma=gamma, min_child_weight=min_child_weight, base_score=base_score, seed=random_state)\n",
        "Validate_XGBoost(model, X_val, y_val)\n",
        "model.save_model(\"ML_algorithms/Stacking_faults_manual_model.dat\")"
      ],
      "metadata": {
        "id": "j9BnBTMmxAeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Plot how important the features (atoms) are in the structure and why they are important"
      ],
      "metadata": {
        "id": "vdzC_Bk9xF86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer, shap_values = shap_essential_figure(model, X_train.values)"
      ],
      "metadata": {
        "id": "JH_rc78KxGIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Block Sequence"
      ],
      "metadata": {
        "id": "bNRLBYvTxJzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the layer contribution value for each layer\n",
        "Block_sequence_list = []\n",
        "for i in range(4, 4+1*Layers):\n",
        "    keep_atoms = np.sum((shap_values[np.where(X_train.values[:,i] == 2),i]))\n",
        "    remove_atoms = np.sum((shap_values[np.where(X_train.values[:,i] == 1),i]))\n",
        "    Block_sequence_list.append(keep_atoms - remove_atoms)\n",
        "\n",
        "lower_percentile = 0\n",
        "print (\"Best Sequences:\", np.where((np.array(Block_sequence_list) < lower_percentile) & (np.array(Block_sequence_list) < 0)))\n",
        "print (\"Worst Sequences:\", np.where((np.array(Block_sequence_list) > lower_percentile) & (np.array(Block_sequence_list) > 0)))\n",
        "\n",
        "count_b = 0\n",
        "count_g = 0\n",
        "count_r = 0\n",
        "for i in range(Layers-1):\n",
        "    if Block_sequence_list[i] == 0:\n",
        "        color = \"k\"\n",
        "        label = \"Does not Matter\"\n",
        "        plt.plot(i, 0, \"o\", color=color, markersize=10, label=label if count_b == 0 else \"_nolegend_\")\n",
        "        count_b += 1\n",
        "    elif Block_sequence_list[i] < lower_percentile:\n",
        "        color = \"g\"\n",
        "        label = \"Does like Layer\"\n",
        "        plt.plot(i, 0, \"o\", color=color, markersize=10, label=label if count_g == 0 else \"_nolegend_\")\n",
        "        count_g += 1\n",
        "    elif Block_sequence_list[i] > lower_percentile:\n",
        "        color = \"r\"\n",
        "        label = \"Does not like Layer\"\n",
        "        plt.plot(i, 0, \"o\", color=color, markersize=10, label=label if count_r == 0 else \"_nolegend_\")\n",
        "        count_r += 1\n",
        "\n",
        "# Plot the results\n",
        "plt.yticks([], [])\n",
        "plt.legend()\n",
        "plt.title(\"Block_sequence\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LlC2vMKYxJ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intergrowth sequence"
      ],
      "metadata": {
        "id": "ecMWlyX3xOfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the layer contribution value for each layer\n",
        "Intergrowth_sequence_list = []\n",
        "for i in range(4+Layers, 4+2*Layers-1):\n",
        "    keep_atoms = np.sum((shap_values[np.where(X_train.values[:,i] == 1),i]))\n",
        "    remove_atoms = np.sum((shap_values[np.where(X_train.values[:,i] == 0),i]))\n",
        "    Intergrowth_sequence_list.append(keep_atoms - remove_atoms)\n",
        "\n",
        "lower_percentile = 0\n",
        "print (\"Best Sequences:\", np.where((np.array(Intergrowth_sequence_list) < lower_percentile) & (np.array(Intergrowth_sequence_list) < 0)))\n",
        "print (\"Worst Sequences:\", np.where((np.array(Intergrowth_sequence_list) > lower_percentile) & (np.array(Intergrowth_sequence_list) > 0)))\n",
        "\n",
        "count_b = 0\n",
        "count_g = 0\n",
        "count_r = 0\n",
        "for i in range(Layers-1):\n",
        "    if Intergrowth_sequence_list[i] == 0:\n",
        "        color = \"k\"\n",
        "        label = \"Does not Matter\"\n",
        "        plt.plot(i, 0, \"o\", color=color, marker=\"s\", markersize=10, label=label if count_b == 0 else \"_nolegend_\")\n",
        "        count_b += 1\n",
        "    elif Intergrowth_sequence_list[i] < lower_percentile:\n",
        "        color = \"g\"\n",
        "        label = \"Does like R - Want to change Layer\"\n",
        "        plt.plot(i, 0, \"o\", color='dodgerblue', marker=\"s\", markersize=10, label=label if count_g == 0 else \"_nolegend_\")\n",
        "        count_g += 1\n",
        "    elif Intergrowth_sequence_list[i] > lower_percentile:\n",
        "        color = \"r\"\n",
        "        label = \"Does not like R - Do not want to change Layer\"\n",
        "        plt.plot(i, 0, \"o\", color=color, marker=\"s\", markersize=10, label=label if count_r == 0 else \"_nolegend_\")\n",
        "        count_r += 1\n",
        "        \n",
        "# Plot the results\n",
        "plt.yticks([], [])\n",
        "plt.legend()\n",
        "plt.title(\"Intergrowth_sequence\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7DgVLojzxOpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the stacking fault distributions"
      ],
      "metadata": {
        "id": "dcO6DmMnxTRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Pyr_blocks = []\n",
        "Ram_blocks = []\n",
        "for i in range(len(Intergrowth_sequence_list)):\n",
        "    if Intergrowth_sequence_list[i]>lower_percentile:\n",
        "        Pyr_blocks.append(i)\n",
        "    if Intergrowth_sequence_list[i]<lower_percentile:\n",
        "        Ram_blocks.append(i)\n",
        "print(Pyr_blocks)\n",
        "list_DSD_pyr = np.zeros(20)        #plot number of domains of a given size\n",
        "list_DSD_pyr_freq = np.zeros(20)   #plot frequence of domains with a given size\n",
        "list_DSD_pyr_vol = np.zeros(20)    #plot frequence of domain related to size\n",
        "list_DSD_ram = np.zeros(20)\n",
        "list_DSD_ram_freq = np.zeros(20)\n",
        "list_DSD_ram_vol = np.zeros(20)\n",
        "block_arr = np.linspace(0, 19, 20)\n",
        "x_array_pyr = np.array([i*0.44 for i in range(0, 20)]) \n",
        "x_array_ram = np.array([i*0.466 for i in range(0, 20)]) \n",
        "\n",
        "i=0\n",
        "block = 1\n",
        "freq_Pyr=0\n",
        "vol_Pyr=0\n",
        "while i<len(Pyr_blocks)-1:\n",
        "    if Pyr_blocks[i+1]==Pyr_blocks[i]+1:\n",
        "        block +=1\n",
        "    if Pyr_blocks[i+1]!=Pyr_blocks[i]+1:\n",
        "        list_DSD_pyr[block+1]+=1\n",
        "        block = 1\n",
        "    i+=1\n",
        "freq_Pyr = np.sum(list_DSD_pyr)\n",
        "for j in range(len(list_DSD_pyr)):\n",
        "    list_DSD_pyr_freq[j]=list_DSD_pyr[j]/freq_Pyr\n",
        "    vol_Pyr+=list_DSD_pyr[j]*x_array_pyr[j]\n",
        "for k in range(len(list_DSD_pyr)):\n",
        "    list_DSD_pyr_vol[k]=list_DSD_pyr[k]*x_array_pyr[k]/vol_Pyr\n",
        "    \n",
        "sigma_count = np.ones(20)*1.5\n",
        "sigma_count += np.sqrt(list_DSD_pyr)\n",
        "sigma_vol = sigma_count*x_array_pyr/vol_Pyr\n",
        "\n",
        "    \n",
        "\n",
        "i=0\n",
        "block = 1 \n",
        "freq_Ram = 0\n",
        "vol_Ram=0\n",
        "while i<len(Ram_blocks)-1:\n",
        "    if Ram_blocks[i+1]==Ram_blocks[i]+1:\n",
        "        block +=1\n",
        "    if Ram_blocks[i+1]!=Ram_blocks[i]+1:\n",
        "        list_DSD_ram[block+1]+=1\n",
        "        block = 1\n",
        "    i+=1\n",
        "freq_Ram = np.sum(list_DSD_ram)\n",
        "for j in range(len(list_DSD_ram)):\n",
        "    list_DSD_ram_freq[j]=list_DSD_ram[j]/freq_Pyr\n",
        "    vol_Ram+=list_DSD_ram[j]*x_array_ram[j]\n",
        "for k in range(len(list_DSD_ram)):\n",
        "    list_DSD_ram_vol[k]=list_DSD_ram[k]*x_array_ram[k]/vol_Ram\n",
        "\n",
        "print(x_array_pyr)\n",
        "print(list_DSD_ram_vol)\n",
        "\n",
        "plt.plot(x_array_pyr, list_DSD_pyr_vol)\n",
        "plt.plot(x_array_ram, list_DSD_ram_vol)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WRlMt6yvxTxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}